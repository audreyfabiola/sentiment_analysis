{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import psutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "from memory_profiler import profile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B07W7CTLD1</td>\n",
       "      <td>Mamaearth-Onion-Growth-Control-Redensyl</td>\n",
       "      <td>2019-09-06</td>\n",
       "      <td>1</td>\n",
       "      <td>I bought this hair oil after viewing so many g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B07W7CTLD1</td>\n",
       "      <td>Mamaearth-Onion-Growth-Control-Redensyl</td>\n",
       "      <td>2019-08-14</td>\n",
       "      <td>5</td>\n",
       "      <td>Used This Mama Earth Newly Launched Onion Oil ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B07W7CTLD1</td>\n",
       "      <td>Mamaearth-Onion-Growth-Control-Redensyl</td>\n",
       "      <td>2019-10-19</td>\n",
       "      <td>1</td>\n",
       "      <td>So bad product...My hair falling increase too ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B07W7CTLD1</td>\n",
       "      <td>Mamaearth-Onion-Growth-Control-Redensyl</td>\n",
       "      <td>2019-09-16</td>\n",
       "      <td>1</td>\n",
       "      <td>Product just smells similar to navarathna hair...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B07W7CTLD1</td>\n",
       "      <td>Mamaearth-Onion-Growth-Control-Redensyl</td>\n",
       "      <td>2019-08-18</td>\n",
       "      <td>5</td>\n",
       "      <td>I have been trying different onion oil for my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>B0818ZYVH1</td>\n",
       "      <td>Patanjali-Kanti-Natural-Cleanser-Shampoo</td>\n",
       "      <td>2017-12-03</td>\n",
       "      <td>5</td>\n",
       "      <td>Made in bharat and we r proud of it...no harmf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>B0818ZYVH1</td>\n",
       "      <td>Patanjali-Kanti-Natural-Cleanser-Shampoo</td>\n",
       "      <td>2020-05-09</td>\n",
       "      <td>5</td>\n",
       "      <td>Superb shampoo so soft n delicate on hair. Bee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>B0818ZYVH1</td>\n",
       "      <td>Patanjali-Kanti-Natural-Cleanser-Shampoo</td>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>5</td>\n",
       "      <td>Wonderful product. Using little produces enoug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>B0818ZYVH1</td>\n",
       "      <td>Patanjali-Kanti-Natural-Cleanser-Shampoo</td>\n",
       "      <td>2019-04-03</td>\n",
       "      <td>5</td>\n",
       "      <td>already used earlier. good item. no issue i fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>B0818ZYVH1</td>\n",
       "      <td>Patanjali-Kanti-Natural-Cleanser-Shampoo</td>\n",
       "      <td>2017-07-24</td>\n",
       "      <td>5</td>\n",
       "      <td>It's not highly perfumed, does wat an ayurvedi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           asin                                      name        date  rating  \\\n",
       "0    B07W7CTLD1   Mamaearth-Onion-Growth-Control-Redensyl  2019-09-06       1   \n",
       "1    B07W7CTLD1   Mamaearth-Onion-Growth-Control-Redensyl  2019-08-14       5   \n",
       "2    B07W7CTLD1   Mamaearth-Onion-Growth-Control-Redensyl  2019-10-19       1   \n",
       "3    B07W7CTLD1   Mamaearth-Onion-Growth-Control-Redensyl  2019-09-16       1   \n",
       "4    B07W7CTLD1   Mamaearth-Onion-Growth-Control-Redensyl  2019-08-18       5   \n",
       "..          ...                                       ...         ...     ...   \n",
       "995  B0818ZYVH1  Patanjali-Kanti-Natural-Cleanser-Shampoo  2017-12-03       5   \n",
       "996  B0818ZYVH1  Patanjali-Kanti-Natural-Cleanser-Shampoo  2020-05-09       5   \n",
       "997  B0818ZYVH1  Patanjali-Kanti-Natural-Cleanser-Shampoo  2016-10-01       5   \n",
       "998  B0818ZYVH1  Patanjali-Kanti-Natural-Cleanser-Shampoo  2019-04-03       5   \n",
       "999  B0818ZYVH1  Patanjali-Kanti-Natural-Cleanser-Shampoo  2017-07-24       5   \n",
       "\n",
       "                                                review  \n",
       "0    I bought this hair oil after viewing so many g...  \n",
       "1    Used This Mama Earth Newly Launched Onion Oil ...  \n",
       "2    So bad product...My hair falling increase too ...  \n",
       "3    Product just smells similar to navarathna hair...  \n",
       "4    I have been trying different onion oil for my ...  \n",
       "..                                                 ...  \n",
       "995  Made in bharat and we r proud of it...no harmf...  \n",
       "996  Superb shampoo so soft n delicate on hair. Bee...  \n",
       "997  Wonderful product. Using little produces enoug...  \n",
       "998  already used earlier. good item. no issue i fe...  \n",
       "999  It's not highly perfumed, does wat an ayurvedi...  \n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv and put into dataframe\n",
    "df = pd.read_csv(\"amazon_reviews_3k.csv\", encoding=\"UTF-8\", nrows=1000)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data and Sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = df['review'].values  \n",
    "\n",
    "processed_reviews = []\n",
    "\n",
    "# iterate through each review\n",
    "for review in range(0, len(x_values)):\n",
    "    # remove special characters\n",
    "    processed_review = re.sub(r'\\W', ' ', str(df['review'][review]))\n",
    "\n",
    "    # remove single characters\n",
    "    processed_review = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_review)\n",
    "    processed_review = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_review)\n",
    "\n",
    "    # remove multiple spaces\n",
    "    processed_review = re.sub(r'\\s+', ' ', processed_review, flags=re.I)\n",
    "\n",
    "    # remove prefixed 'b' (if applicable, assuming X is a list of strings)\n",
    "    processed_review = re.sub(r'^b\\s+', '', processed_review)\n",
    "\n",
    "    # convert to lowercase\n",
    "    processed_review = processed_review.lower()\n",
    "\n",
    "    # append to the empty list created earlier\n",
    "    processed_reviews.append(processed_review)\n",
    "\n",
    "# put all processed reviews into new column \n",
    "df['processed_reviews'] = processed_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing stop words:\n",
      "0    i bought this hair oil after viewing so many g...\n",
      "1    used this mama earth newly launched onion oil ...\n",
      "2    so bad product my hair falling increase too mu...\n",
      "3    product just smells similar to navarathna hair...\n",
      "4    i have been trying different onion oil for my ...\n",
      "Name: processed_reviews, dtype: object\n",
      "\n",
      "After removing stop words:\n",
      "0    bought hair oil viewing many good comments pro...\n",
      "1    used mama earth newly launched onion oil twice...\n",
      "2    bad product hair falling increase much order s...\n",
      "3    product smells similar navarathna hair oil str...\n",
      "4    trying different onion oil hair hair healthy p...\n",
      "Name: clean_review_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# set stopwords\n",
    "stopWords = set(stopwords.words('english') + ['the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from'])\n",
    "\n",
    "def removeStopWords(stopWords, rvw_txt):\n",
    "    newtxt = ' '.join([word for word in rvw_txt.split() if word.lower() not in stopWords])\n",
    "    return newtxt\n",
    "\n",
    "\n",
    "df['processed_reviews'] = df['processed_reviews'].astype(str)\n",
    "\n",
    "# before stop words removed\n",
    "print(\"Before removing stop words:\")\n",
    "print(df['processed_reviews'].head())\n",
    "\n",
    "# apply removeStopWords function\n",
    "df['clean_review_text'] = [removeStopWords(stopWords, x) for x in df['processed_reviews']]\n",
    "\n",
    "# after stop words removed\n",
    "print(\"\\nAfter removing stop words:\")\n",
    "print(df['clean_review_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model = SentimentIntensityAnalyzer()\n",
    "sentiment_scores=[]\n",
    "sentiment_score_flag = []\n",
    "\n",
    "# iterate over text and calculate sentiment\n",
    "for text in df['clean_review_text']:\n",
    "        sentimentResults = sentiment_model.polarity_scores(text)\n",
    "        sentiment_score = sentimentResults[\"compound\"]\n",
    "\n",
    "        # append sentiment score and label\n",
    "        sentiment_scores.append(sentiment_score)\n",
    "\n",
    "        # marking the sentiments as positive, negative and neutral \n",
    "        if sentimentResults['compound'] >= 0.05 : \n",
    "            sentiment_score_flag.append('positive')\n",
    "  \n",
    "        elif sentimentResults['compound'] <= - 0.05 : \n",
    "            sentiment_score_flag.append('negative')\n",
    "  \n",
    "        else : \n",
    "            sentiment_score_flag.append('neutral')\n",
    "            \n",
    "# add into new column\n",
    "df['scores'] = sentiment_scores\n",
    "df['scoreStatus'] = sentiment_score_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text\n",
    "def tokenize(text):\n",
    "    # obtains tokens with a least 1 alphabet\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())\n",
    "\n",
    "# create a mapping between words and their corresponding IDs\n",
    "def mapping(tokens):\n",
    "    word_to_id = dict()\n",
    "    id_to_word = dict()\n",
    "\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "# generate training data for a word embedding model using a skip-gram approach\n",
    "def generate_training_data(tokens, word_to_id, window_size):\n",
    "    N = len(tokens)\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(N):\n",
    "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
    "                   list(range(i + 1, min(N, i + window_size + 1)))\n",
    "        for j in nbr_inds:\n",
    "            X.append(word_to_id[tokens[i]])\n",
    "            Y.append(word_to_id[tokens[j]])\n",
    "            \n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    Y = np.array(Y)\n",
    "    Y = np.expand_dims(Y, axis=0)\n",
    "            \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_review_text'] = df['clean_review_text'].astype(str)\n",
    "docs = df['clean_review_text'].tolist()\n",
    "\n",
    "# tokenize each document separately\n",
    "tokens = [tokenize(doc) for doc in docs]\n",
    "\n",
    "# create word-to-id and id-to-word mappings from all tokens\n",
    "word_to_id, id_to_word = mapping([token for sublist in tokens for token in sublist])\n",
    "\n",
    "# generate training data from tokens\n",
    "X, Y = generate_training_data([token for sublist in tokens for token in sublist], word_to_id, 3)\n",
    "\n",
    "# vocabulary size\n",
    "vocab_size = len(id_to_word)\n",
    "\n",
    "# number of training samples\n",
    "m = Y.shape[1]\n",
    "\n",
    "# turn Y into one hot encoding\n",
    "Y_one_hot = np.zeros((vocab_size, m))\n",
    "Y_one_hot[Y.flatten(), np.arange(m)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the word embedding matrix\n",
    "def initialize_wrd_emb(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    vocab_size: int. vocabulary size of your corpus or training data\n",
    "    emb_size: int. word embedding size. How many dimensions to represent each vocabulary\n",
    "    \"\"\"\n",
    "    WRD_EMB = np.random.randn(vocab_size, emb_size) * 0.01\n",
    "    \n",
    "    assert(WRD_EMB.shape == (vocab_size, emb_size))\n",
    "    return WRD_EMB\n",
    "\n",
    "# initialize the weight matrix for a dense layer\n",
    "def initialize_dense(input_size, output_size):\n",
    "    \"\"\"\n",
    "    input_size: int. size of the input to the dense layer\n",
    "    output_szie: int. size of the output out of the dense layer\n",
    "    \"\"\"\n",
    "    W = np.random.randn(output_size, input_size) * 0.01\n",
    "    \n",
    "    assert(W.shape == (output_size, input_size))\n",
    "    return W\n",
    "\n",
    "# initialize parameters for a neural network model\n",
    "def initialize_parameters(vocab_size, emb_size):\n",
    "    WRD_EMB = initialize_wrd_emb(vocab_size, emb_size)\n",
    "    W = initialize_dense(emb_size, vocab_size)\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['WRD_EMB'] = WRD_EMB\n",
    "    parameters['W'] = W\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert word indices to word vectors\n",
    "def ind_to_word_vecs(inds, parameters):\n",
    "    \"\"\"\n",
    "    inds: numpy array. shape: (1, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = inds.shape[1]\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    word_vec = WRD_EMB[inds.flatten(), :].T\n",
    "    \n",
    "    assert(word_vec.shape == (WRD_EMB.shape[1], m))\n",
    "    \n",
    "    return word_vec\n",
    "\n",
    "# perform linear transformation with a dense layer\n",
    "def linear_dense(word_vec, parameters):\n",
    "    \"\"\"\n",
    "    word_vec: numpy array. shape: (emb_size, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = word_vec.shape[1]\n",
    "    W = parameters['W']\n",
    "    Z = np.dot(W, word_vec)\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], m))\n",
    "    \n",
    "    return W, Z\n",
    "\n",
    "# apply softmax activation to the output\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Z: output out of the dense layer. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0, keepdims=True) + 0.001)\n",
    "    \n",
    "    assert(softmax_out.shape == Z.shape)\n",
    "\n",
    "    return softmax_out\n",
    "\n",
    "# perform forward propagation through the neural network\n",
    "def forward_propagation(inds, parameters):\n",
    "    word_vec = ind_to_word_vecs(inds, parameters)\n",
    "    W, Z = linear_dense(word_vec, parameters)\n",
    "    softmax_out = softmax(Z)\n",
    "    \n",
    "    caches = {}\n",
    "    caches['inds'] = inds\n",
    "    caches['word_vec'] = word_vec\n",
    "    caches['W'] = W\n",
    "    caches['Z'] = Z\n",
    "    \n",
    "    return softmax_out, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the cross-entropy loss\n",
    "def cross_entropy(softmax_out, Y):\n",
    "    \"\"\"\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    m = softmax_out.shape[1]\n",
    "    cost = -(1 / m) * np.sum(np.sum(Y * np.log(softmax_out + 0.001), axis=0, keepdims=True), axis=1)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute gradient of softmax activation\n",
    "def softmax_backward(Y, softmax_out):\n",
    "    \"\"\"\n",
    "    Y: labels of training data. shape: (vocab_size, m)\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    dL_dZ = softmax_out - Y\n",
    "    \n",
    "    assert(dL_dZ.shape == softmax_out.shape)\n",
    "    return dL_dZ\n",
    "\n",
    "# compute gradient of the dense layer\n",
    "def dense_backward(dL_dZ, caches):\n",
    "    \"\"\"\n",
    "    dL_dZ: shape: (vocab_size, m)\n",
    "    caches: dict. results from each steps of forward propagation\n",
    "    \"\"\"\n",
    "    W = caches['W']\n",
    "    word_vec = caches['word_vec']\n",
    "    m = word_vec.shape[1]\n",
    "    \n",
    "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vec.T)\n",
    "    dL_dword_vec = np.dot(W.T, dL_dZ)\n",
    "\n",
    "    assert(W.shape == dL_dW.shape)\n",
    "    assert(word_vec.shape == dL_dword_vec.shape)\n",
    "    \n",
    "    return dL_dW, dL_dword_vec\n",
    "\n",
    "# perform backward propagation to compute gradients\n",
    "def backward_propagation(Y, softmax_out, caches):\n",
    "    dL_dZ = softmax_backward(Y, softmax_out)\n",
    "    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n",
    "    \n",
    "    gradients = dict()\n",
    "    gradients['dL_dZ'] = dL_dZ\n",
    "    gradients['dL_dW'] = dL_dW\n",
    "    gradients['dL_dword_vec'] = dL_dword_vec\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# update the parameters using gradient descent\n",
    "def update_parameters(parameters, caches, gradients, learning_rate):\n",
    "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
    "    inds = caches['inds']\n",
    "    dL_dword_vec = gradients['dL_dword_vec']\n",
    "    m = inds.shape[-1]\n",
    "    \n",
    "    parameters['WRD_EMB'][inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
    "\n",
    "    parameters['W'] -= learning_rate * gradients['dL_dW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement training for skip-gram word embedding model \n",
    "def skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, parameters=None, print_cost=False, plot_cost=True):\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters(vocab_size, emb_size)\n",
    "    \n",
    "    begin_time = datetime.now()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        batch_inds = list(range(0, m, batch_size))\n",
    "        np.random.shuffle(batch_inds)\n",
    "        for i in batch_inds:\n",
    "            X_batch = X[:, i:i+batch_size]\n",
    "            Y_batch = Y[:, i:i+batch_size]\n",
    "\n",
    "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
    "            gradients = backward_propagation(Y_batch, softmax_out, caches)\n",
    "            update_parameters(parameters, caches, gradients, learning_rate)\n",
    "            cost = cross_entropy(softmax_out, Y_batch)\n",
    "            epoch_cost += np.squeeze(cost)\n",
    "            \n",
    "        costs.append(epoch_cost)\n",
    "    end_time = datetime.now()\n",
    "    print('training time: {}'.format(end_time - begin_time))\n",
    "            \n",
    "    if plot_cost:\n",
    "        plt.plot(np.arange(epochs), costs)\n",
    "        plt.xlabel('# of epochs')\n",
    "        plt.ylabel('cost')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 0:18:24.995090\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paras = skipgram_model_training(X, Y_one_hot, vocab_size, 50, 0.05, 10, batch_size=64, parameters=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find file /var/folders/k7/hgkz89px7jz91sbyc7lxqdxh0000gn/T/ipykernel_45609/3161904966.py\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     paras \u001b[38;5;241m=\u001b[39m skipgram_model_training(X, Y_one_hot, vocab_size, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \n\u001b[1;32m     12\u001b[0m     print_memory_usage()   \n\u001b[0;32m---> 14\u001b[0m train_skipgram_model_memory()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/memory_profiler.py:1188\u001b[0m, in \u001b[0;36mprofile.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(wrapped\u001b[38;5;241m=\u001b[39mfunc)\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1187\u001b[0m     prof \u001b[38;5;241m=\u001b[39m get_prof()\n\u001b[0;32m-> 1188\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[43mprof\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m     show_results_bound(prof)\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m val\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/memory_profiler.py:761\u001b[0m, in \u001b[0;36mLineProfiler.wrap_function.<locals>.f\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_ctxmgr():\n\u001b[0;32m--> 761\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [14], line 11\u001b[0m, in \u001b[0;36mtrain_skipgram_model_memory\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;129m@profile\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_skipgram_model_memory\u001b[39m():\n\u001b[0;32m---> 11\u001b[0m     paras \u001b[38;5;241m=\u001b[39m \u001b[43mskipgram_model_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_one_hot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m \n\u001b[1;32m     12\u001b[0m     print_memory_usage()\n",
      "Cell \u001b[0;32mIn [12], line 18\u001b[0m, in \u001b[0;36mskipgram_model_training\u001b[0;34m(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size, parameters, print_cost, plot_cost)\u001b[0m\n\u001b[1;32m     15\u001b[0m X_batch \u001b[38;5;241m=\u001b[39m X[:, i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m     16\u001b[0m Y_batch \u001b[38;5;241m=\u001b[39m Y[:, i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[0;32m---> 18\u001b[0m softmax_out, caches \u001b[38;5;241m=\u001b[39m \u001b[43mforward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m gradients \u001b[38;5;241m=\u001b[39m backward_propagation(Y_batch, softmax_out, caches)\n\u001b[1;32m     20\u001b[0m update_parameters(parameters, caches, gradients, learning_rate)\n",
      "Cell \u001b[0;32mIn [9], line 43\u001b[0m, in \u001b[0;36mforward_propagation\u001b[0;34m(inds, parameters)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_propagation\u001b[39m(inds, parameters):\n\u001b[1;32m     42\u001b[0m     word_vec \u001b[38;5;241m=\u001b[39m ind_to_word_vecs(inds, parameters)\n\u001b[0;32m---> 43\u001b[0m     W, Z \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_vec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     softmax_out \u001b[38;5;241m=\u001b[39m softmax(Z)\n\u001b[1;32m     46\u001b[0m     caches \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn [9], line 23\u001b[0m, in \u001b[0;36mlinear_dense\u001b[0;34m(word_vec, parameters)\u001b[0m\n\u001b[1;32m     21\u001b[0m m \u001b[38;5;241m=\u001b[39m word_vec\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     22\u001b[0m W \u001b[38;5;241m=\u001b[39m parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 23\u001b[0m Z \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_vec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(Z\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (W\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], m))\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m W, Z\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# check memory usage of applying TF-IDF to the data\n",
    "def print_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    memory_info = process.memory_info()\n",
    "\n",
    "    print(f\"Memory Usage: {memory_info.rss / (1024 ** 2):.2f} MB (Resident Set Size)\")\n",
    "    print(f\"Virtual Memory: {memory_info.vms / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "@profile\n",
    "def train_skipgram_model_memory():\n",
    "    paras = skipgram_model_training(X, Y_one_hot, vocab_size, 50, 0.05, 10, batch_size=64, parameters=None) \n",
    "    print_memory_usage()   \n",
    "\n",
    "train_skipgram_model_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain word embeddings for each word in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate word embeddings \n",
    "def get_word_embeddings(words, parameters):\n",
    "    word_indices = [word_to_id[word] for word in words if word in word_to_id]\n",
    "    word_indices = np.array(word_indices).astype(int)  # Convert to integers\n",
    "    word_vecs = ind_to_word_vecs(word_indices.reshape(1, -1), parameters)\n",
    "    return word_vecs\n",
    "\n",
    "word_embeddings = get_word_embeddings(docs, paras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates average sentence embeddings from tokenized texts\n",
    "def aggregate_embeddings(tokenized_texts, parameters):\n",
    "    sentence_embeddings = []\n",
    "    placeholder_embedding = None\n",
    "\n",
    "    for sentence in tokenized_texts:\n",
    "        word_indices = [word_to_id[word] for word in sentence if word in word_to_id]\n",
    "        word_indices = np.array(word_indices, dtype=np.int64)  # convert to integer type\n",
    "\n",
    "        if len(word_indices) > 0:\n",
    "            word_vecs = ind_to_word_vecs(word_indices.reshape(1, -1), parameters)\n",
    "            avg_embedding = np.mean(word_vecs, axis=1)  # average word embeddings for the sentence\n",
    "            sentence_embeddings.append(avg_embedding)\n",
    "            placeholder_embedding = avg_embedding  # update the placeholder\n",
    "\n",
    "        elif placeholder_embedding is not None:\n",
    "            # append the placeholder for empty sentences\n",
    "            sentence_embeddings.append(placeholder_embedding)\n",
    "\n",
    "    return np.vstack(sentence_embeddings)\n",
    "\n",
    "sentence_embeddings = aggregate_embeddings(tokens, paras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sentence_embeddings  # features\n",
    "y_train = df[\"scoreStatus\"]  # labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.76\n",
      "Precision: 0.76\n",
      "Recall: 0.76\n",
      "F1-Score: 0.76\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.05      0.10        40\n",
      "     neutral       0.00      0.00      0.00        10\n",
      "    positive       0.76      1.00      0.86       150\n",
      "\n",
      "    accuracy                           0.76       200\n",
      "   macro avg       0.59      0.35      0.32       200\n",
      "weighted avg       0.77      0.76      0.67       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/priscillaabigail/Library/Python/3.9/lib/python/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# initialize the classifier\n",
    "clf = LinearSVC()\n",
    "\n",
    "# train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on test set: {accuracy}\")\n",
    "\n",
    "# calculate precision, recall, and F1-score\n",
    "precision = precision_score(y_test, y_pred, average='micro')  # Choose the appropriate average setting\n",
    "recall = recall_score(y_test, y_pred, average='micro')  # Choose the appropriate average setting\n",
    "f1 = f1_score(y_test, y_pred, average='micro')  # Choose the appropriate average setting\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-Score: {f1}')\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(y_test, y_pred, zero_division=0)\n",
    "print(\"\\n\",report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.4375\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.21      0.52      0.30        25\n",
      "     neutral       0.10      0.30      0.15        10\n",
      "    positive       0.81      0.43      0.56       125\n",
      "\n",
      "    accuracy                           0.44       160\n",
      "   macro avg       0.37      0.42      0.34       160\n",
      "weighted avg       0.67      0.44      0.50       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# initialize the classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# train the classifier\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on test set: {accuracy}\")\n",
    "\n",
    "# classification report\n",
    "report = classification_report(y_test, y_pred, zero_division=0)\n",
    "print(\"\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 0.05100107192993164 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/priscillaabigail/Library/Python/3.9/lib/python/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Code for training your model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Training Time: {training_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 4.57763671875e-05 MB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "model_size = sys.getsizeof(clf)\n",
    "print(f\"Model Size: {model_size / (1024 * 1024)} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment: ['negative']\n"
     ]
    }
   ],
   "source": [
    "new_text = \"this product was very bad\"\n",
    "tokenized_new_text = new_text.split()\n",
    "\n",
    "new_text_word_indices = [word_to_id[word] for word in tokenized_new_text if word in word_to_id]\n",
    "new_text_word_vecs = ind_to_word_vecs(np.array(new_text_word_indices).reshape(1, -1), paras)\n",
    "\n",
    "avg_embedding_new_text = np.mean(new_text_word_vecs, axis=1)\n",
    "\n",
    "predicted_sentiment = clf.predict(avg_embedding_new_text.reshape(1, -1))\n",
    "print(f\"Predicted sentiment: {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment: ['positive']\n"
     ]
    }
   ],
   "source": [
    "new_text = \"this product was very good\"\n",
    "tokenized_new_text = new_text.split()\n",
    "\n",
    "new_text_word_indices = [word_to_id[word] for word in tokenized_new_text if word in word_to_id]\n",
    "new_text_word_vecs = ind_to_word_vecs(np.array(new_text_word_indices).reshape(1, -1), paras)\n",
    "\n",
    "avg_embedding_new_text = np.mean(new_text_word_vecs, axis=1)\n",
    "\n",
    "predicted_sentiment = clf.predict(avg_embedding_new_text.reshape(1, -1))\n",
    "print(f\"Predicted sentiment: {predicted_sentiment}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
