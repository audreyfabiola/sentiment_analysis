{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import psutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "from memory_profiler import profile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>rating</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B07W7CTLD1</td>\n",
       "      <td>Mamaearth-Onion-Growth-Control-Redensyl</td>\n",
       "      <td>2019-09-06</td>\n",
       "      <td>1</td>\n",
       "      <td>I bought this hair oil after viewing so many g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B07W7CTLD1</td>\n",
       "      <td>Mamaearth-Onion-Growth-Control-Redensyl</td>\n",
       "      <td>2019-08-14</td>\n",
       "      <td>5</td>\n",
       "      <td>Used This Mama Earth Newly Launched Onion Oil ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B07W7CTLD1</td>\n",
       "      <td>Mamaearth-Onion-Growth-Control-Redensyl</td>\n",
       "      <td>2019-10-19</td>\n",
       "      <td>1</td>\n",
       "      <td>So bad product...My hair falling increase too ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B07W7CTLD1</td>\n",
       "      <td>Mamaearth-Onion-Growth-Control-Redensyl</td>\n",
       "      <td>2019-09-16</td>\n",
       "      <td>1</td>\n",
       "      <td>Product just smells similar to navarathna hair...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B07W7CTLD1</td>\n",
       "      <td>Mamaearth-Onion-Growth-Control-Redensyl</td>\n",
       "      <td>2019-08-18</td>\n",
       "      <td>5</td>\n",
       "      <td>I have been trying different onion oil for my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>B00ISNVQMW</td>\n",
       "      <td>Titan-Octane-Analog-Silver-Watch-NK1650BM03</td>\n",
       "      <td>2019-09-30</td>\n",
       "      <td>5</td>\n",
       "      <td>After HMT, titan is first choice for all young...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>B00ISNVQMW</td>\n",
       "      <td>Titan-Octane-Analog-Silver-Watch-NK1650BM03</td>\n",
       "      <td>2019-10-30</td>\n",
       "      <td>5</td>\n",
       "      <td>Premium and genuine product at this price poin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>B00ISNVQMW</td>\n",
       "      <td>Titan-Octane-Analog-Silver-Watch-NK1650BM03</td>\n",
       "      <td>2019-10-28</td>\n",
       "      <td>5</td>\n",
       "      <td>Its nice. I purchase for my sister to give as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>B00ISNVQMW</td>\n",
       "      <td>Titan-Octane-Analog-Silver-Watch-NK1650BM03</td>\n",
       "      <td>2019-08-11</td>\n",
       "      <td>5</td>\n",
       "      <td>Very good watch for college or university goin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>B00ISNVQMW</td>\n",
       "      <td>Titan-Octane-Analog-Silver-Watch-NK1650BM03</td>\n",
       "      <td>2019-07-29</td>\n",
       "      <td>5</td>\n",
       "      <td>Nice watch...i gifted this watch to my dad on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           asin                                         name        date  \\\n",
       "0    B07W7CTLD1      Mamaearth-Onion-Growth-Control-Redensyl  2019-09-06   \n",
       "1    B07W7CTLD1      Mamaearth-Onion-Growth-Control-Redensyl  2019-08-14   \n",
       "2    B07W7CTLD1      Mamaearth-Onion-Growth-Control-Redensyl  2019-10-19   \n",
       "3    B07W7CTLD1      Mamaearth-Onion-Growth-Control-Redensyl  2019-09-16   \n",
       "4    B07W7CTLD1      Mamaearth-Onion-Growth-Control-Redensyl  2019-08-18   \n",
       "..          ...                                          ...         ...   \n",
       "495  B00ISNVQMW  Titan-Octane-Analog-Silver-Watch-NK1650BM03  2019-09-30   \n",
       "496  B00ISNVQMW  Titan-Octane-Analog-Silver-Watch-NK1650BM03  2019-10-30   \n",
       "497  B00ISNVQMW  Titan-Octane-Analog-Silver-Watch-NK1650BM03  2019-10-28   \n",
       "498  B00ISNVQMW  Titan-Octane-Analog-Silver-Watch-NK1650BM03  2019-08-11   \n",
       "499  B00ISNVQMW  Titan-Octane-Analog-Silver-Watch-NK1650BM03  2019-07-29   \n",
       "\n",
       "     rating                                             review  \n",
       "0         1  I bought this hair oil after viewing so many g...  \n",
       "1         5  Used This Mama Earth Newly Launched Onion Oil ...  \n",
       "2         1  So bad product...My hair falling increase too ...  \n",
       "3         1  Product just smells similar to navarathna hair...  \n",
       "4         5  I have been trying different onion oil for my ...  \n",
       "..      ...                                                ...  \n",
       "495       5  After HMT, titan is first choice for all young...  \n",
       "496       5  Premium and genuine product at this price poin...  \n",
       "497       5  Its nice. I purchase for my sister to give as ...  \n",
       "498       5  Very good watch for college or university goin...  \n",
       "499       5  Nice watch...i gifted this watch to my dad on ...  \n",
       "\n",
       "[500 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read csv and put into dataframe\n",
    "df = pd.read_csv(\"amazon_reviews_3k.csv\", encoding=\"UTF-8\", nrows=500)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data and Sentiment Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_values = df['review'].values  \n",
    "\n",
    "processed_reviews = []\n",
    "\n",
    "# iterate through each review\n",
    "for review in range(0, len(x_values)):\n",
    "    # remove special characters\n",
    "    processed_review = re.sub(r'\\W', ' ', str(df['review'][review]))\n",
    "\n",
    "    # remove single characters\n",
    "    processed_review = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_review)\n",
    "    processed_review = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_review)\n",
    "\n",
    "    # remove multiple spaces\n",
    "    processed_review = re.sub(r'\\s+', ' ', processed_review, flags=re.I)\n",
    "\n",
    "    # remove prefixed 'b' (if applicable, assuming X is a list of strings)\n",
    "    processed_review = re.sub(r'^b\\s+', '', processed_review)\n",
    "\n",
    "    # convert to lowercase\n",
    "    processed_review = processed_review.lower()\n",
    "\n",
    "    # append to the empty list created earlier\n",
    "    processed_reviews.append(processed_review)\n",
    "\n",
    "# put all processed reviews into new column \n",
    "df['processed_reviews'] = processed_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing stop words:\n",
      "0    i bought this hair oil after viewing so many g...\n",
      "1    used this mama earth newly launched onion oil ...\n",
      "2    so bad product my hair falling increase too mu...\n",
      "3    product just smells similar to navarathna hair...\n",
      "4    i have been trying different onion oil for my ...\n",
      "Name: processed_reviews, dtype: object\n",
      "\n",
      "After removing stop words:\n",
      "0    bought hair oil viewing many good comments pro...\n",
      "1    used mama earth newly launched onion oil twice...\n",
      "2    bad product hair falling increase much order s...\n",
      "3    product smells similar navarathna hair oil str...\n",
      "4    trying different onion oil hair hair healthy p...\n",
      "Name: clean_review_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# set stopwords\n",
    "stopWords = set(stopwords.words('english') + ['the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from'])\n",
    "\n",
    "def removeStopWords(stopWords, rvw_txt):\n",
    "    newtxt = ' '.join([word for word in rvw_txt.split() if word.lower() not in stopWords])\n",
    "    return newtxt\n",
    "\n",
    "\n",
    "df['processed_reviews'] = df['processed_reviews'].astype(str)\n",
    "\n",
    "# before stop words removed\n",
    "print(\"Before removing stop words:\")\n",
    "print(df['processed_reviews'].head())\n",
    "\n",
    "# apply removeStopWords function\n",
    "df['clean_review_text'] = [removeStopWords(stopWords, x) for x in df['processed_reviews']]\n",
    "\n",
    "# after stop words removed\n",
    "print(\"\\nAfter removing stop words:\")\n",
    "print(df['clean_review_text'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_model = SentimentIntensityAnalyzer()\n",
    "sentiment_scores=[]\n",
    "sentiment_score_flag = []\n",
    "\n",
    "# iterate over text and calculate sentiment\n",
    "for text in df['clean_review_text']:\n",
    "        sentimentResults = sentiment_model.polarity_scores(text)\n",
    "        sentiment_score = sentimentResults[\"compound\"]\n",
    "\n",
    "        # append sentiment score and label\n",
    "        sentiment_scores.append(sentiment_score)\n",
    "\n",
    "        # marking the sentiments as positive, negative and neutral \n",
    "        if sentimentResults['compound'] >= 0.05 : \n",
    "            sentiment_score_flag.append('positive')\n",
    "  \n",
    "        elif sentimentResults['compound'] <= - 0.05 : \n",
    "            sentiment_score_flag.append('negative')\n",
    "  \n",
    "        else : \n",
    "            sentiment_score_flag.append('neutral')\n",
    "            \n",
    "# add into new column\n",
    "df['scores'] = sentiment_scores\n",
    "df['scoreStatus'] = sentiment_score_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Generate Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text\n",
    "def tokenize(text):\n",
    "    # obtains tokens with a least 1 alphabet\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())\n",
    "\n",
    "# create a mapping between words and their corresponding IDs\n",
    "def mapping(tokens):\n",
    "    word_to_id = dict()\n",
    "    id_to_word = dict()\n",
    "\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "# generate training data for a word embedding model using a skip-gram approach\n",
    "def generate_training_data(tokens, word_to_id, window_size):\n",
    "    N = len(tokens)\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(N):\n",
    "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
    "                   list(range(i + 1, min(N, i + window_size + 1)))\n",
    "        for j in nbr_inds:\n",
    "            X.append(word_to_id[tokens[i]])\n",
    "            Y.append(word_to_id[tokens[j]])\n",
    "            \n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    Y = np.array(Y)\n",
    "    Y = np.expand_dims(Y, axis=0)\n",
    "            \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_review_text'] = df['clean_review_text'].astype(str)\n",
    "docs = df['clean_review_text'].tolist()\n",
    "\n",
    "# tokenize each document separately\n",
    "tokens = [tokenize(doc) for doc in docs]\n",
    "\n",
    "# create word-to-id and id-to-word mappings from all tokens\n",
    "word_to_id, id_to_word = mapping([token for sublist in tokens for token in sublist])\n",
    "\n",
    "# generate training data from tokens\n",
    "X, Y = generate_training_data([token for sublist in tokens for token in sublist], word_to_id, 3)\n",
    "\n",
    "# vocabulary size\n",
    "vocab_size = len(id_to_word)\n",
    "\n",
    "# number of training samples\n",
    "m = Y.shape[1]\n",
    "\n",
    "# turn Y into one hot encoding\n",
    "Y_one_hot = np.zeros((vocab_size, m))\n",
    "Y_one_hot[Y.flatten(), np.arange(m)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the word embedding matrix\n",
    "def initialize_wrd_emb(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    vocab_size: int. vocabulary size of your corpus or training data\n",
    "    emb_size: int. word embedding size. How many dimensions to represent each vocabulary\n",
    "    \"\"\"\n",
    "    WRD_EMB = np.random.randn(vocab_size, emb_size) * 0.01\n",
    "    \n",
    "    assert(WRD_EMB.shape == (vocab_size, emb_size))\n",
    "    return WRD_EMB\n",
    "\n",
    "# initialize the weight matrix for a dense layer\n",
    "def initialize_dense(input_size, output_size):\n",
    "    \"\"\"\n",
    "    input_size: int. size of the input to the dense layer\n",
    "    output_szie: int. size of the output out of the dense layer\n",
    "    \"\"\"\n",
    "    W = np.random.randn(output_size, input_size) * 0.01\n",
    "    \n",
    "    assert(W.shape == (output_size, input_size))\n",
    "    return W\n",
    "\n",
    "# initialize parameters for a neural network model\n",
    "def initialize_parameters(vocab_size, emb_size):\n",
    "    WRD_EMB = initialize_wrd_emb(vocab_size, emb_size)\n",
    "    W = initialize_dense(emb_size, vocab_size)\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['WRD_EMB'] = WRD_EMB\n",
    "    parameters['W'] = W\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert word indices to word vectors\n",
    "def ind_to_word_vecs(inds, parameters):\n",
    "    \"\"\"\n",
    "    inds: numpy array. shape: (1, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = inds.shape[1]\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    word_vec = WRD_EMB[inds.flatten(), :].T\n",
    "    \n",
    "    assert(word_vec.shape == (WRD_EMB.shape[1], m))\n",
    "    \n",
    "    return word_vec\n",
    "\n",
    "# perform linear transformation with a dense layer\n",
    "def linear_dense(word_vec, parameters):\n",
    "    \"\"\"\n",
    "    word_vec: numpy array. shape: (emb_size, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = word_vec.shape[1]\n",
    "    W = parameters['W']\n",
    "    Z = np.dot(W, word_vec)\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], m))\n",
    "    \n",
    "    return W, Z\n",
    "\n",
    "# apply softmax activation to the output\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Z: output out of the dense layer. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0, keepdims=True) + 0.001)\n",
    "    \n",
    "    assert(softmax_out.shape == Z.shape)\n",
    "\n",
    "    return softmax_out\n",
    "\n",
    "# perform forward propagation through the neural network\n",
    "def forward_propagation(inds, parameters):\n",
    "    word_vec = ind_to_word_vecs(inds, parameters)\n",
    "    W, Z = linear_dense(word_vec, parameters)\n",
    "    softmax_out = softmax(Z)\n",
    "    \n",
    "    caches = {}\n",
    "    caches['inds'] = inds\n",
    "    caches['word_vec'] = word_vec\n",
    "    caches['W'] = W\n",
    "    caches['Z'] = Z\n",
    "    \n",
    "    return softmax_out, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the cross-entropy loss\n",
    "def cross_entropy(softmax_out, Y):\n",
    "    \"\"\"\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    m = softmax_out.shape[1]\n",
    "    cost = -(1 / m) * np.sum(np.sum(Y * np.log(softmax_out + 0.001), axis=0, keepdims=True), axis=1)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute gradient of softmax activation\n",
    "def softmax_backward(Y, softmax_out):\n",
    "    \"\"\"\n",
    "    Y: labels of training data. shape: (vocab_size, m)\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    dL_dZ = softmax_out - Y\n",
    "    \n",
    "    assert(dL_dZ.shape == softmax_out.shape)\n",
    "    return dL_dZ\n",
    "\n",
    "# compute gradient of the dense layer\n",
    "def dense_backward(dL_dZ, caches):\n",
    "    \"\"\"\n",
    "    dL_dZ: shape: (vocab_size, m)\n",
    "    caches: dict. results from each steps of forward propagation\n",
    "    \"\"\"\n",
    "    W = caches['W']\n",
    "    word_vec = caches['word_vec']\n",
    "    m = word_vec.shape[1]\n",
    "    \n",
    "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vec.T)\n",
    "    dL_dword_vec = np.dot(W.T, dL_dZ)\n",
    "\n",
    "    assert(W.shape == dL_dW.shape)\n",
    "    assert(word_vec.shape == dL_dword_vec.shape)\n",
    "    \n",
    "    return dL_dW, dL_dword_vec\n",
    "\n",
    "# perform backward propagation to compute gradients\n",
    "def backward_propagation(Y, softmax_out, caches):\n",
    "    dL_dZ = softmax_backward(Y, softmax_out)\n",
    "    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n",
    "    \n",
    "    gradients = dict()\n",
    "    gradients['dL_dZ'] = dL_dZ\n",
    "    gradients['dL_dW'] = dL_dW\n",
    "    gradients['dL_dword_vec'] = dL_dword_vec\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# update the parameters using gradient descent\n",
    "def update_parameters(parameters, caches, gradients, learning_rate):\n",
    "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
    "    inds = caches['inds']\n",
    "    dL_dword_vec = gradients['dL_dword_vec']\n",
    "    m = inds.shape[-1]\n",
    "    \n",
    "    parameters['WRD_EMB'][inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
    "\n",
    "    parameters['W'] -= learning_rate * gradients['dL_dW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement training for skip-gram word embedding model \n",
    "def skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, parameters=None, print_cost=False, plot_cost=True):\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters(vocab_size, emb_size)\n",
    "    \n",
    "    begin_time = datetime.now()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        batch_inds = list(range(0, m, batch_size))\n",
    "        np.random.shuffle(batch_inds)\n",
    "        for i in batch_inds:\n",
    "            X_batch = X[:, i:i+batch_size]\n",
    "            Y_batch = Y[:, i:i+batch_size]\n",
    "\n",
    "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
    "            gradients = backward_propagation(Y_batch, softmax_out, caches)\n",
    "            update_parameters(parameters, caches, gradients, learning_rate)\n",
    "            cost = cross_entropy(softmax_out, Y_batch)\n",
    "            epoch_cost += np.squeeze(cost)\n",
    "            \n",
    "        costs.append(epoch_cost)\n",
    "    end_time = datetime.now()\n",
    "    print('training time: {}'.format(end_time - begin_time))\n",
    "            \n",
    "    if plot_cost:\n",
    "        plt.plot(np.arange(epochs), costs)\n",
    "        plt.xlabel('# of epochs')\n",
    "        plt.ylabel('cost')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time: 0:08:03.999801\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paras = skipgram_model_training(X, Y_one_hot, vocab_size, 50, 0.05, 10, batch_size=64, parameters=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find file /var/folders/k7/hgkz89px7jz91sbyc7lxqdxh0000gn/T/ipykernel_45501/3161904966.py\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     paras \u001b[38;5;241m=\u001b[39m skipgram_model_training(X, Y_one_hot, vocab_size, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m10\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \n\u001b[1;32m     12\u001b[0m     print_memory_usage()   \n\u001b[0;32m---> 14\u001b[0m train_skipgram_model_memory()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/memory_profiler.py:1188\u001b[0m, in \u001b[0;36mprofile.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(wrapped\u001b[38;5;241m=\u001b[39mfunc)\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1187\u001b[0m     prof \u001b[38;5;241m=\u001b[39m get_prof()\n\u001b[0;32m-> 1188\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[43mprof\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1189\u001b[0m     show_results_bound(prof)\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m val\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/memory_profiler.py:761\u001b[0m, in \u001b[0;36mLineProfiler.wrap_function.<locals>.f\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_ctxmgr():\n\u001b[0;32m--> 761\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [14], line 11\u001b[0m, in \u001b[0;36mtrain_skipgram_model_memory\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;129m@profile\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_skipgram_model_memory\u001b[39m():\n\u001b[0;32m---> 11\u001b[0m     paras \u001b[38;5;241m=\u001b[39m \u001b[43mskipgram_model_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_one_hot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m \n\u001b[1;32m     12\u001b[0m     print_memory_usage()\n",
      "Cell \u001b[0;32mIn [12], line 19\u001b[0m, in \u001b[0;36mskipgram_model_training\u001b[0;34m(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size, parameters, print_cost, plot_cost)\u001b[0m\n\u001b[1;32m     16\u001b[0m Y_batch \u001b[38;5;241m=\u001b[39m Y[:, i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m     18\u001b[0m softmax_out, caches \u001b[38;5;241m=\u001b[39m forward_propagation(X_batch, parameters)\n\u001b[0;32m---> 19\u001b[0m gradients \u001b[38;5;241m=\u001b[39m \u001b[43mbackward_propagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoftmax_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m update_parameters(parameters, caches, gradients, learning_rate)\n\u001b[1;32m     21\u001b[0m cost \u001b[38;5;241m=\u001b[39m cross_entropy(softmax_out, Y_batch)\n",
      "Cell \u001b[0;32mIn [11], line 32\u001b[0m, in \u001b[0;36mbackward_propagation\u001b[0;34m(Y, softmax_out, caches)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward_propagation\u001b[39m(Y, softmax_out, caches):\n\u001b[0;32m---> 32\u001b[0m     dL_dZ \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoftmax_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     dL_dW, dL_dword_vec \u001b[38;5;241m=\u001b[39m dense_backward(dL_dZ, caches)\n\u001b[1;32m     35\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n",
      "Cell \u001b[0;32mIn [11], line 7\u001b[0m, in \u001b[0;36msoftmax_backward\u001b[0;34m(Y, softmax_out)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msoftmax_backward\u001b[39m(Y, softmax_out):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    Y: labels of training data. shape: (vocab_size, m)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    softmax_out: output out of softmax. shape: (vocab_size, m)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     dL_dZ \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax_out\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m(dL_dZ\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m softmax_out\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dL_dZ\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# check memory usage of applying TF-IDF to the data\n",
    "def print_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    memory_info = process.memory_info()\n",
    "\n",
    "    print(f\"Memory Usage: {memory_info.rss / (1024 ** 2):.2f} MB (Resident Set Size)\")\n",
    "    print(f\"Virtual Memory: {memory_info.vms / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "@profile\n",
    "def train_skipgram_model_memory():\n",
    "    paras = skipgram_model_training(X, Y_one_hot, vocab_size, 50, 0.05, 10, batch_size=64, parameters=None) \n",
    "    print_memory_usage()   \n",
    "\n",
    "train_skipgram_model_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain word embeddings for each word in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate word embeddings \n",
    "def get_word_embeddings(words, parameters):\n",
    "    word_indices = [word_to_id[word] for word in words if word in word_to_id]\n",
    "    word_indices = np.array(word_indices).astype(int)  # Convert to integers\n",
    "    word_vecs = ind_to_word_vecs(word_indices.reshape(1, -1), parameters)\n",
    "    return word_vecs\n",
    "\n",
    "word_embeddings = get_word_embeddings(docs, paras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates average sentence embeddings from tokenized texts\n",
    "def aggregate_embeddings(tokenized_texts, parameters):\n",
    "    sentence_embeddings = []\n",
    "    placeholder_embedding = None\n",
    "\n",
    "    for sentence in tokenized_texts:\n",
    "        word_indices = [word_to_id[word] for word in sentence if word in word_to_id]\n",
    "        word_indices = np.array(word_indices, dtype=np.int64)  # convert to integer type\n",
    "\n",
    "        if len(word_indices) > 0:\n",
    "            word_vecs = ind_to_word_vecs(word_indices.reshape(1, -1), parameters)\n",
    "            avg_embedding = np.mean(word_vecs, axis=1)  # average word embeddings for the sentence\n",
    "            sentence_embeddings.append(avg_embedding)\n",
    "            placeholder_embedding = avg_embedding  # update the placeholder\n",
    "\n",
    "        elif placeholder_embedding is not None:\n",
    "            # append the placeholder for empty sentences\n",
    "            sentence_embeddings.append(placeholder_embedding)\n",
    "\n",
    "    return np.vstack(sentence_embeddings)\n",
    "\n",
    "sentence_embeddings = aggregate_embeddings(tokens, paras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sentence_embeddings  # features\n",
    "y_train = df[\"scoreStatus\"]  # labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.81\n",
      "Precision: 0.81\n",
      "Recall: 0.81\n",
      "F1-Score: 0.81\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        16\n",
      "     neutral       0.00      0.00      0.00         3\n",
      "    positive       0.81      1.00      0.90        81\n",
      "\n",
      "    accuracy                           0.81       100\n",
      "   macro avg       0.27      0.33      0.30       100\n",
      "weighted avg       0.66      0.81      0.72       100\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/priscillaabigail/Library/Python/3.9/lib/python/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# initialize the classifier\n",
    "clf = LinearSVC()\n",
    "\n",
    "# train the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on test set: {accuracy}\")\n",
    "\n",
    "# calculate precision, recall, and F1-score\n",
    "precision = precision_score(y_test, y_pred, average='micro')  # Choose the appropriate average setting\n",
    "recall = recall_score(y_test, y_pred, average='micro')  # Choose the appropriate average setting\n",
    "f1 = f1_score(y_test, y_pred, average='micro')  # Choose the appropriate average setting\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1-Score: {f1}')\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(y_test, y_pred, zero_division=0)\n",
    "print(\"\\n\",report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.45\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.40      0.67      0.50        21\n",
      "     neutral       0.05      1.00      0.09         1\n",
      "    positive       0.91      0.36      0.52        58\n",
      "\n",
      "    accuracy                           0.45        80\n",
      "   macro avg       0.45      0.68      0.37        80\n",
      "weighted avg       0.77      0.45      0.51        80\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# initialize the classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# train the classifier\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test set\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on test set: {accuracy}\")\n",
    "\n",
    "# classification report\n",
    "report = classification_report(y_test, y_pred, zero_division=0)\n",
    "print(\"\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 0.0318148136138916 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/priscillaabigail/Library/Python/3.9/lib/python/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Code for training your model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "print(f\"Training Time: {training_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Size: 4.57763671875e-05 MB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "model_size = sys.getsizeof(clf)\n",
    "print(f\"Model Size: {model_size / (1024 * 1024)} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment: ['positive']\n"
     ]
    }
   ],
   "source": [
    "new_text = \"this product was very bad\"\n",
    "tokenized_new_text = new_text.split()\n",
    "\n",
    "new_text_word_indices = [word_to_id[word] for word in tokenized_new_text if word in word_to_id]\n",
    "new_text_word_vecs = ind_to_word_vecs(np.array(new_text_word_indices).reshape(1, -1), paras)\n",
    "\n",
    "avg_embedding_new_text = np.mean(new_text_word_vecs, axis=1)\n",
    "\n",
    "predicted_sentiment = clf.predict(avg_embedding_new_text.reshape(1, -1))\n",
    "print(f\"Predicted sentiment: {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment: ['positive']\n"
     ]
    }
   ],
   "source": [
    "new_text = \"this product was very good\"\n",
    "tokenized_new_text = new_text.split()\n",
    "\n",
    "new_text_word_indices = [word_to_id[word] for word in tokenized_new_text if word in word_to_id]\n",
    "new_text_word_vecs = ind_to_word_vecs(np.array(new_text_word_indices).reshape(1, -1), paras)\n",
    "\n",
    "avg_embedding_new_text = np.mean(new_text_word_vecs, axis=1)\n",
    "\n",
    "predicted_sentiment = clf.predict(avg_embedding_new_text.reshape(1, -1))\n",
    "print(f\"Predicted sentiment: {predicted_sentiment}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
